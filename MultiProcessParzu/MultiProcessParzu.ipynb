{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f345a6e9",
   "metadata": {},
   "source": [
    "## MultiProcessParzu \n",
    "\n",
    "Here we initialize multiple ParZu docker containers and send requests to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab7e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install --upgrade pip\n",
    "! python -m pip install docker nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESTART_EVERY = 3\n",
    "FILEPATH=\"data/swissdox_test.tsv\"\n",
    "CHUNKSIZE=100\n",
    "N_CONTAINERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32825839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "client = docker.from_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.containers.list(filters={'ancestor': 'stancerserver:latest'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da43b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ../../external_repos/stancer_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21437097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "# a python function that gets the absolute path from a relative path relative to the cwd\n",
    "def get_abs_path_from_rel_path(rel_path):\n",
    "    commands = '''\n",
    "    cd ../../external_repos/stancer_setup\n",
    "    pwd\n",
    "    '''.encode('utf-8')\n",
    "    process = subprocess.Popen('/bin/bash', stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    out, err = process.communicate(commands)\n",
    "    return out\n",
    "    # get rows of file\n",
    "    # output = subprocess.run(['cd', '../../external_repos/stancer_setup', '&&', 'pwd'], \n",
    "    #     capture_output=True, \n",
    "    #     text=True, \n",
    "    #     shell=True\n",
    "    # )\n",
    "    # # wc_output.stdout will be of format ` N_lines filename`\n",
    "    # # subtract 1 to remove header\n",
    "    # # print(str(wc_output.stdout).split(\" \"))\n",
    "    # return str(output.stdout)\n",
    "\n",
    "abspath_stancerserver = get_abs_path_from_rel_path('../../external_repos/stancer_setup')[:-1].decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce16872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# container = client.containers.run('stancerserver', \n",
    "#     '/bin/bash ./scripts/run_server.sh', \n",
    "#     volumes=[f'{ abspath_stancerserver }:/app'], \n",
    "#     ports={\"5003\": \"5004\"}, \n",
    "#     detach=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e240d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# container.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize containers\n",
    "containers = []\n",
    "portnos = []\n",
    "\n",
    "problem_containers = client.containers.list(filters={'ancestor': 'stancerserver:latest'})\n",
    "\n",
    "if problem_containers:\n",
    "    for c in problem_containers:\n",
    "        c.stop()\n",
    "        # c.remove()\n",
    "\n",
    "for i in range(0, N_CONTAINERS):\n",
    "    portno = f\"500{i+4}\"\n",
    "\n",
    "    container = client.containers.run('stancerserver', \n",
    "        '/bin/bash ./scripts/run_server.sh', \n",
    "        volumes=[f'{ abspath_stancerserver }:/app'], \n",
    "        ports={\"5003\": portno}, \n",
    "        detach=True\n",
    "        )\n",
    "\n",
    "    for x in container.logs(stream=True):\n",
    "        print(x)\n",
    "        if \"Running\" in x.decode(\"utf-8\"):\n",
    "            break\n",
    "    containers.append(container)\n",
    "    portnos.append(portno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.containers.list(filters={'ancestor': 'stancerserver:latest'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aab47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "# download the punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def clean_text_from_xml(text):\n",
    "    # logging.info(f\"Clean text { text }\")\n",
    "\n",
    "    soup = BeautifulSoup(text, \"xml\")\n",
    "\n",
    "    # breakpoint()\n",
    "\n",
    "    textstr = \" \".join([x.get_text() for x in soup.findAll('p')])\n",
    "\n",
    "    # remove sentences with multicap words.\n",
    "    tokenized = nltk.tokenize.sent_tokenize(textstr, language='german')\n",
    "    tokenized = [x for x in tokenized if not re.search(r\"([a-zäöü][A-ZÄÖÜ]){1,}\", x)]\n",
    "    \n",
    "    # discard all sentences at the end that don't end with a dot.\n",
    "    for i in range(len(tokenized)-1, 0, -1):\n",
    "        sent_tokenized = nltk.tokenize.word_tokenize(tokenized[i], language='german', preserve_line=False)\n",
    "        sent_length = len(sent_tokenized)\n",
    "        if tokenized[i][-1] != \".\" or sent_length < 4:\n",
    "            tokenized.pop(i)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return \" \".join(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a91f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# import logging \n",
    "url = f\"http://localhost:5004/parse/\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"text\": \"Ich bin Berliner.\",\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data, timeout=200)\n",
    "\n",
    "logging.info(f\"make_request - status code:  { response.status_code }\")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import threading\n",
    "\n",
    "class Status(Enum):\n",
    "    FREE = \"FREE\"\n",
    "    BUSY = \"BUSY\"\n",
    "\n",
    "class ContainerWrapper:\n",
    "    def __init__(self, container):\n",
    "        self.container = container\n",
    "        # self.status = Status.FREE\n",
    "        self.runs = 0\n",
    "        self.restarts = 0\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def restart_container(self):\n",
    "        logging.info(\"Container will restart, waiting\")\n",
    "        self.restarts += 1\n",
    "        running_counts = self.restarts + 1\n",
    "        self.container.restart()\n",
    "        for x in self.container.logs(stream=True):\n",
    "            logging.info(x.decode(\"utf-8\"))\n",
    "            count_string = \"Running on all addresses (0.0.0.0)\".lower()\n",
    "            if count_string in x.decode(\"utf-8\").lower():\n",
    "                running_counts -= 1\n",
    "                if running_counts == 0:\n",
    "                    logging.info(\"Container is being running again\")\n",
    "                    break\n",
    "                logging.info(f\"Found running signal waiting for more signal, still {running_counts} to go\")\n",
    "        logging.info(\"Container has restarted\")\n",
    "        self.runs = 0\n",
    "\n",
    "containerwrappers = [ContainerWrapper(container) for container in containers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# import logging\n",
    "import concurrent\n",
    "import random\n",
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "# a function that checks whether a given text-string is in Conll format\n",
    "def is_conll(text):\n",
    "    if len(text.split(\"\\n\")) > 1 and text.split(\"\\n\")[0].startswith(\"1\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def make_request(text, port):\n",
    "\n",
    "    logging.info(f\"portno:  { port }\")\n",
    "\n",
    "    logging.info(f\"text:  { text[:500] if text else '' }\")\n",
    "\n",
    "    url = f\"http://localhost:{port}/parse/\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"text\": text,\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data, timeout=200)\n",
    "\n",
    "    logging.info(f\"make_request - status code:  { response.status_code }\")\n",
    "\n",
    "    # logging.info(f\"make_request - reading response text:  { response.text }\")\n",
    "\n",
    "    if response.status_code == 200 and is_conll(response.text):\n",
    "        return response.text\n",
    "    return None\n",
    "\n",
    "class BusyError(Exception):\n",
    "    pass \n",
    "\n",
    "def retry(f):\n",
    "    @wraps(f)\n",
    "    def wrapped(*args, **kwargs):\n",
    "        while True:\n",
    "            sleep(3)\n",
    "            try:\n",
    "                return f(*args, **kwargs)\n",
    "            except BusyError:\n",
    "                logging.info(\"Container is busy, retrying\")\n",
    "                pass\n",
    "    return wrapped\n",
    "\n",
    "@retry\n",
    "def process_texts(text_df, containerwrapper, port, header=False):\n",
    "\n",
    "    if containerwrapper.lock.locked():\n",
    "        raise BusyError(\"Container is busy\")\n",
    "\n",
    "    containerwrapper.lock.acquire(blocking=True)\n",
    "\n",
    "    containerwrapper.runs += 1\n",
    "\n",
    "    if containerwrapper.runs > RESTART_EVERY:\n",
    "        logging.info(str(\"Container has passed \"+  str(RESTART_EVERY)+ \" runs, restarting\"))\n",
    "        # containerwrapper.restart_container()\n",
    "        # containerwrapper.runs = 0\n",
    "        containerwrapper.restart_container()\n",
    "        # containerwrapper.container.restart()\n",
    "        # time.sleep(20)\n",
    "\n",
    "\n",
    "    conlls = []\n",
    "    cleaned_texts = []\n",
    "    for row in text_df.iterrows():\n",
    "        logging.info(\"Iterating over row\")\n",
    "        text = row[1][\"content\"]\n",
    "        cleaned_text = clean_text_from_xml(text)\n",
    "        try:\n",
    "            conll = make_request(cleaned_text, port)\n",
    "            time.sleep(1)\n",
    "            if not conll:\n",
    "                for i in range(5):\n",
    "                    logging.info(f\"make_request returned empty string, trying again { i }\")\n",
    "                    conll = make_request(cleaned_text, port)\n",
    "                    if conll:\n",
    "                        break\n",
    "        except:\n",
    "            # deal with a problem in the return (timeout problem)\n",
    "            # potentially restart the container and check until works again!\n",
    "            # exponential backoff\n",
    "            logging.info(\"Receving of object failed %d. Returning (close thread)\", 1)\n",
    "            conll = None\n",
    "            pass\n",
    "        logging.info(f\"Logging conll [1st line]:  { conll[:200] if conll else 'none' }\")\n",
    "        conlls.append(conll)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    text_df[\"content_conll\"] = conlls\n",
    "    text_df[\"content_xml\"] = text_df[\"content\"].to_list()\n",
    "    text_df[\"content\"] = cleaned_texts\n",
    "    text_df.to_csv(\"data/parsed.csv\", index=False, mode=\"a\", header=header)\n",
    "\n",
    "    containerwrapper.lock.release()\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00abd0aa",
   "metadata": {},
   "source": [
    "### Read a dataframe in chunked style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadcc9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if file exists and if variable RESTORE_MODE is set to false, then delete the file\n",
    "import os\n",
    "\n",
    "RESTORE_MODE = False\n",
    "if os.path.isfile('data/parsed.csv') and not RESTORE_MODE:\n",
    "    os.remove('data/parsed.csv')\n",
    "\n",
    "# if not in restore mode write first line (header) to file\n",
    "# if not RESTORE_MODE:\n",
    "#     open('data/parsed.csv', 'w').write('id,pubtime,medium_code,medium_name,rubric,regional,doctype,doctype_description,language,char_count,dateline,head,subhead,content_id,content,content_xml,content_conll\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e2fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_data(filepath, chunksize=CHUNKSIZE):\n",
    "    df_generator = pd.read_csv(filepath, delimiter=\"\\t\", chunksize=chunksize, index_col=0)\n",
    "    # if len(df) != len(df.drop_duplicates(subset=[\"content\"])):\n",
    "    #     print(\"Duplicate texts found.\")\n",
    "    #     sys.exit(0)\n",
    "    return df_generator\n",
    "\n",
    "# a function that returns a generator that circulates through a list\n",
    "# def cycle(lst):\n",
    "#     i = 0\n",
    "#     while True:\n",
    "#         yield lst[i]\n",
    "#         i = (i + 1) % len(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc52b4",
   "metadata": {},
   "source": [
    "### Run threaded execution pool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f224400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_rows(filename):\n",
    "    # get rows of file\n",
    "    wc_output = subprocess.run(['wc','-l', f'{filename}'], capture_output=True, text=True)\n",
    "    # wc_output.stdout will be of format ` N_lines filename`\n",
    "    # subtract 1 to remove header\n",
    "    # print(str(wc_output.stdout).split(\" \"))\n",
    "    return int(wc_output.stdout.split()[0]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.info(\"Testing update. Starting value is %d.\", database.value)\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import subprocess\n",
    "import concurrent\n",
    "import random\n",
    "from functools import wraps\n",
    "from time import sleep\n",
    "\n",
    "# format = \"%(asctime)s: %(message)s\"\n",
    "# logging.basicConfig(format=format, level=logging.INFO, datefmt=\"%H:%M:%S\")\n",
    "\n",
    "no_rows = get_no_rows(FILEPATH)\n",
    "\n",
    "itertimes = no_rows // CHUNKSIZE\n",
    "\n",
    "# write first \n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='mpparzu.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.INFO)\n",
    "# logger.setLevel(logging.WARNING)\n",
    "\n",
    "# create shared container datastructure\n",
    "# containerstats = { container.id: 0 for container in containers }\n",
    "\n",
    "def run(my_iter, itertimes):\n",
    "    # process first non parallel\n",
    "    df, cont, portno = next(my_iter)\n",
    "    process_texts(df, cont, portno, header=True)\n",
    "    with tqdm(total=itertimes) as pbar:\n",
    "        # let's give it some more threads:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=N_CONTAINERS) as executor:\n",
    "            futures = {executor.submit(process_texts, df, cont, portno): 0 for df, cont, portno in my_iter}\n",
    "            results = {}\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                arg = futures[future]\n",
    "                results[arg] = future.result()\n",
    "                pbar.update(1)\n",
    "    # print(321, results[321])\n",
    "\n",
    "df_iter = load_data(FILEPATH)\n",
    "cont_iter = itertools.cycle(containerwrappers)\n",
    "portno_iter = itertools.cycle(portnos)\n",
    "\n",
    "# write indices to list of chunks and skip if in restore mode!\n",
    "\n",
    "my_iter = zip(\n",
    "    df_iter, \n",
    "    cont_iter, \n",
    "    portno_iter\n",
    "    ) \n",
    "\n",
    "run(my_iter, itertimes)\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "#     for index in range(2):\n",
    "#         executor.submit(database.update, index)\n",
    "# logging.info(\"Testing update. Ending value is %d.\", database.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for container in containers:\n",
    "#     container.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for container in containers:\n",
    "    container.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
