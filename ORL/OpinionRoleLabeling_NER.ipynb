{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ecbf02",
   "metadata": {},
   "source": [
    "## Opinion Role Labeling\n",
    "\n",
    "### Using the NER approach\n",
    "-------\n",
    "\n",
    "**Requirements**\n",
    "```\n",
    "!pip install transformers datasets evaluate seqeval\n",
    "```\n",
    "\n",
    "alternatively use the following command (on *anaconda*):\n",
    "```\n",
    "!conda install -y -c conda-forge transformers datasets evaluate seqeval\n",
    "```\n",
    "\n",
    "**Load a dataset in the following format**\n",
    "```\n",
    "[CLS] Das immer wieder mit dem Separatismus liebäugelnde französischsprachige Quebec wollte sich mit dem Verfassungskompromiss , TARGET die neun vorwiegend englischsprachigen HOLDER schliesslich zugestimmt hatten , nicht abfinden . [SEP] \n",
    "[CLS] Das immer wieder mit dem Separatismus liebäugelnde französischsprachige Quebec wollte sich mit dem Verfassungskompromiss , dem die neun vorwiegend englischsprachigen Provinzen schliesslich zugestimmt hatten , nicht abfinden . [SEP] \n",
    "[CLS] Von den beiden grossen Gewerkschaften hatte die HOLDER ( Unión General del Trabajo ) dieses TARGET unterstützt , während die Comisiones Obreras ( CCOO ) es im Vorfeld abgelehnt hatten , sich jetzt aber mit dem Resultat abfinden . [SEP] \n",
    "[CLS] Von den beiden grossen Gewerkschaften hatte die UGT ( Unión General del Trabajo ) dieses Vorhaben unterstützt , während die Comisiones Obreras ( CCOO ) es im Vorfeld abgelehnt hatten , sich jetzt aber mit dem Resultat abfinden . [SEP] \n",
    "[CLS] Der HOLDER hat keine TARGET verletzt . » Petkovic lehnt laut italienischen Medien auch ein Angebot ab , mit 30 Prozent der Summe abgefunden zu werden . [SEP] \n",
    "[CLS] Der Trainer hat keine Regeln verletzt . » Petkovic lehnt laut italienischen Medien auch ein Angebot ab , mit 30 Prozent der Summe abgefunden zu werden . [SEP] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b698f988-bf43-4ffd-8537-c6e6a5f6cbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/massey/envs/nbdev/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe111bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG VARIABLES\n",
    "\n",
    "# DATASET_PATH=\"../../etl/data/raw/03_holder_target.txt\"\n",
    "\n",
    "TRAIN_DATASET_PATH=\"../../etl/data/processed/ORLConverter/01_train_orl.txt\"\n",
    "VAL_DATASET_PATH=\"../../etl/data/processed/ORLConverter/01_val_orl.txt\"\n",
    "TEST_DATASET_PATH=\"../../etl/data/processed/ORLConverter/01_test_orl.txt\"\n",
    "\n",
    "GENERATE_DATASET=True\n",
    "\n",
    "BASE_MODEL_NAME=\"xlm-roberta-base\"\n",
    "\n",
    "TRAINED_MODEL_NAME=\"./data/trained_model_xlm_roberta_base\"\n",
    "\n",
    "USE_WANDB=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a758b8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=erre_er_component\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=erre_er_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbe029c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in the data: 7666\n",
      "Total rows in the data: 2744\n",
      "Total rows in the data: 2766\n"
     ]
    }
   ],
   "source": [
    "def read_sents(dataset_path):\n",
    "    sents = []\n",
    "\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for c, l in enumerate(f.readlines()):\n",
    "            if c < 100:\n",
    "                pass\n",
    "            sents.append(l.strip())\n",
    "\n",
    "    # Der Minister findet die Debatte langweilig.\n",
    "    real_sents = sents[1::2]\n",
    "\n",
    "    # Der HOLDER findet die TARGET langweilig.\n",
    "    masked_sents = sents[0::2]\n",
    "\n",
    "    assert len(real_sents) == len(masked_sents), \"Nr of masked and real sentences are not equal!\"\n",
    "\n",
    "    print(f\"Total rows in the data: {len(real_sents) + len(masked_sents)}\")\n",
    "    \n",
    "    return real_sents, masked_sents\n",
    "\n",
    "train_sents_real, train_sents_masked = read_sents(TRAIN_DATASET_PATH)\n",
    "val_sents_real, val_sents_masked = read_sents(VAL_DATASET_PATH)\n",
    "test_sents_real, test_sents_masked = read_sents(TEST_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd38cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max holders:  1 Max targets:  1 Max polar expressions:  1\n",
      "Max holders:  1 Max targets:  1 Max polar expressions:  1\n",
      "Max holders:  1 Max targets:  1 Max polar expressions:  1\n",
      "[{'id': 0, 'ner_tags': [0, 1, 3, 0, 0, 2, 0, 0], 'tokens': ['Die', 'Grasshoppers', 'nutzten', 'den', 'personellen', 'Vorteil', 'aus', '.']}, {'id': 1, 'ner_tags': [0, 0, 1, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['Als', 'sich', 'Prinzessin', 'Haya', 'für', 'die', 'jungen', 'Frauen', 'eingesetzt', 'habe', ',', 'sei', 'auch', 'sie', 'ins', 'Visier', 'des', 'Emirs', 'geraten', 'und', 'mit', 'dem', 'Tod', 'bedroht', 'worden', '.']}, {'id': 2, 'ner_tags': [1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], 'tokens': ['Maurer', 'erreichte', 'in', 'dem', 'Departement', ',', 'das', 'er', 'bis', '2015', 'führte', ',', 'eine', 'gewisse', 'Beruhigung', 'der', 'Verhältnisse', '.']}, {'id': 3, 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 2, 0], 'tokens': ['Allerdings', 'reagiert', 'er', 'trotzig', '–', 'und', 'das', 'macht', 'ihn', 'oft', 'nur', 'noch', 'stärker.Cécile', 'Klotzbach', 'Nach', 'jedem', 'Sieg', 'bedankt', 'sich', 'Djokovic', 'mit', 'dieser', 'Geste', 'bei', 'den', 'Fans', '.']}, {'id': 4, 'ner_tags': [0, 2, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['Den', 'Kraftwerksbetreibern', 'fehlen', 'derzeit', 'schlicht', 'die', 'notwendigen', 'Sicherheiten', ',', 'um', 'Investitionen', 'zu', 'tätigen', 'und', 'das', 'Projekt', 'voranzutreiben', '.']}, {'id': 5, 'ner_tags': [0, 3, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['Noch', 'schiesst', 'Jérémie', 'Heitz', 'auf', 'Ski', 'Gipfel', 'hinab', ',', 'die', 'andere', 'mit', 'Eispickeln', 'hochklettern', '.']}, {'id': 6, 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 3, 0], 'tokens': ['Diese', 'Menge', 'dürfte', 'sicherstellen', ',', 'dass', 'auch', 'bei', 'hohem', 'Impftempo', 'bis', 'Mitte', 'März', 'keine', 'Engpässe', 'mehr', 'entstehen', '.']}, {'id': 7, 'ner_tags': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0], 'tokens': ['Die', 'Schweizer', 'Politik', 'und', 'Diplomatie', 'sind', 'weiterhin', 'dringend', 'aufgerufen', ',', 'sich', 'mit', 'Geschick', 'und', 'Überzeugungskraft', 'für', 'einen', 'verdienten', 'Mitbürger', 'einzusetzen', '.']}, {'id': 8, 'ner_tags': [0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0], 'tokens': ['Die', 'Kreation', 'wirkte', 'wie', 'eine', 'mittelmässige', 'Abschlussarbeit', 'eines', 'traditionsbewussten', 'Grafik-Studenten', '–', 'aber', 'nur', 'auf', 'den', 'ersten', 'Blick', '.']}, {'id': 9, 'ner_tags': [0, 0, 3, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['Im', 'Februar', 'klagte', 'das', 'US-Justizdepartement', 'fünf', 'ehemalige', 'Kader', 'des', 'Erdölkonzerns', 'und', 'weitere', 'Personen', 'wegen', 'Bestechung', 'und', 'Geldwäscherei', 'an', '.']}]\n"
     ]
    }
   ],
   "source": [
    "NER_labels = [\"O\", \"HOLDER\", \"TARGET\", \"PEXP\"]\n",
    "\n",
    "test_sent = [\"Peter sowie Lucy mag Katzen sowie auch Hunde .\"]\n",
    "test_sent_masked = [\"HOLDER sowie HOLDER mag TARGET sowie auch TARGET .\"]\n",
    "\n",
    "def align_sentences(real_sentences, masked_sentences, NER_labels):\n",
    "    \"\"\"Given two sentences that have their word-level tokens delimited by a white-space, \n",
    "    create NER-tags for each sentence token.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    max_multiple_holders = 0\n",
    "    max_multiple_targets = 0\n",
    "    max_multiple_pexps = 0\n",
    "\n",
    "    for i, real_sentence, masked_sentence in zip(range(0,len(real_sentences)), real_sentences, masked_sentences):\n",
    "        ner_tags = []\n",
    "        real_sentence_split = real_sentence.split(\" \")\n",
    "        masked_sentence_split = masked_sentence.split(\" \")\n",
    "        \n",
    "        # target / holder counting\n",
    "        target_cnt = 0\n",
    "        holder_cnt = 0\n",
    "        pexp_cnt = 0\n",
    "\n",
    "        assert len(real_sentence_split) == len(masked_sentence_split), \"Misalignment of length of tokens in sentence.\" \\\n",
    "        + str(real_sentence_split) + str(masked_sentence_split) + str(i)\n",
    "\n",
    "        for real_token, masked_token in zip(real_sentence_split, masked_sentence_split):\n",
    "            if real_token == masked_token:\n",
    "                ner_tags.append(0)\n",
    "            elif masked_token == \"HOLDER\":\n",
    "                holder_cnt += 1\n",
    "                tag_index = NER_labels.index(masked_token)\n",
    "                ner_tags.append(tag_index)\n",
    "            elif masked_token == \"TARGET\":\n",
    "                target_cnt += 1\n",
    "                tag_index = NER_labels.index(masked_token)\n",
    "                ner_tags.append(tag_index)\n",
    "            elif masked_token == \"PEXP\":\n",
    "                pexp_cnt += 1\n",
    "                tag_index = NER_labels.index(masked_token)\n",
    "                ner_tags.append(tag_index)\n",
    "\n",
    "        dataset.append({\n",
    "            \"id\": i,\n",
    "            # remove the magic tokens from the sentences, \n",
    "            # since we will pass the sentences through the tokenizer again.\n",
    "            \"ner_tags\": ner_tags[1:-1],\n",
    "            \"tokens\": real_sentence_split[1:-1],\n",
    "        })\n",
    "    max_multiple_holders = max(max_multiple_holders, holder_cnt)\n",
    "    max_multiple_targets = max(max_multiple_targets, target_cnt)\n",
    "    max_multiple_pexps = max(max_multiple_pexps, pexp_cnt)\n",
    "\n",
    "    return dataset, max_multiple_holders, max_multiple_targets, max_multiple_pexps\n",
    "\n",
    "# align for training\n",
    "train_aligned_sents, mmh, mmt, mmp = align_sentences(train_sents_real, train_sents_masked, NER_labels)\n",
    "print(\"Max holders: \", mmh, \"Max targets: \", mmt, \"Max polar expressions: \", mmp)\n",
    "val_aligned_sents, mmh, mmt, mmp = align_sentences(val_sents_real, val_sents_masked, NER_labels)\n",
    "print(\"Max holders: \", mmh, \"Max targets: \", mmt, \"Max polar expressions: \", mmp)\n",
    "test_aligned_sents, mmh, mmt, mmp = align_sentences(test_sents_real, test_sents_masked, NER_labels)\n",
    "print(\"Max holders: \", mmh, \"Max targets: \", mmt, \"Max polar expressions: \", mmp)\n",
    "\n",
    "# UNCOMMENT FOR TEST\n",
    "# aligned_sents, mmh, mmt = align_sentences(test_sent, test_sent_masked, NER_labels)\n",
    "\n",
    "print(train_aligned_sents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24033875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices:  75%|███████▌  | 3/4 [00:00<00:00, 16.93ba/s]\n",
      "Flattening the indices:  50%|█████     | 1/2 [00:00<00:00, 16.11ba/s]\n",
      "Flattening the indices:  50%|█████     | 1/2 [00:00<00:00, 15.97ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'ner_tags', 'tokens'],\n",
      "        num_rows: 3833\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['id', 'ner_tags', 'tokens'],\n",
      "        num_rows: 1372\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'ner_tags', 'tokens'],\n",
      "        num_rows: 1383\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a huggingface dataset out of the record-based dataset\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "\n",
    "if GENERATE_DATASET:\n",
    "    train_dataset = Dataset.from_list(train_aligned_sents)\n",
    "    val_dataset = Dataset.from_list(val_aligned_sents)\n",
    "    test_dataset = Dataset.from_list(test_aligned_sents)\n",
    "\n",
    "    # randomly shuffle\n",
    "    train_dataset = train_dataset.shuffle(seed=42)\n",
    "    val_dataset = val_dataset.shuffle(seed=42)\n",
    "    test_dataset = test_dataset.shuffle(seed=42)\n",
    "\n",
    "    # dataset = dataset.train_test_split(test_size=0.2, shuffle=False)\n",
    "    # train_test_dataset = dataset.train_test_split(test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Split the 10% test + valid in half test, half valid\n",
    "    # test_valid = train_test_dataset['test'].train_test_split(test_size=0.4, shuffle=False)\n",
    "\n",
    "    # gather everyone if you want to have a single DatasetDict\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'valid': val_dataset,\n",
    "        'test': test_dataset})\n",
    "\n",
    "    dataset.save_to_disk(\"./data/split_dataset.hf\")\n",
    "else:\n",
    "    dataset = load_from_disk(\"./data/split_dataset.hf\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe90f4b",
   "metadata": {},
   "source": [
    "**Use DistilBERT tokenizer and embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac02eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f53981c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 2991, 7911, 9, 81405, 42, 45331, 56, 10264, 18370, 444, 615, 18146, 505, 644, 68, 78481, 9026, 74831, 1716, 4383, 933, 6, 190848, 33, 165, 9318, 542, 37616, 2046, 16463, 126, 6, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['<s>', '▁Ein', '▁64', '-', 'jährige', 'r', '▁Amerikan', 'er', '▁hatte', '▁dort', '▁am', '▁1.', '▁Oktober', '▁2017', '▁auf', '▁die', '▁Besucher', '▁eines', '▁Country', 'kon', 'zer', 'ts', '▁', 'geschoss', 'en', '▁und', '▁58', '▁von', '▁ihnen', '▁get', 'öt', 'et', '▁', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# tokenize input\n",
    "tokenized_input = tokenizer(dataset[\"train\"][0][\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "print(tokenized_input)\n",
    "\n",
    "# output subwords\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "005e2f43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_and_align_labels at 0x7fcc309b4268> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      " 75%|███████▌  | 3/4 [00:00<00:00,  6.78ba/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  7.67ba/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  7.15ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'ner_tags', 'tokens', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3833\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'ner_tags', 'tokens', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1372\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'ner_tags', 'tokens', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1383\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        try:\n",
    "            for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "            labels.append(label_ids)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping example due to the following: { str(e) }\")\n",
    "            print(\" \".join(examples[f\"tokens\"][i]))\n",
    "            print(label)\n",
    "            continue\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57b33056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 0, -100, -100, -100, 1, -100, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, -100, -100, -100, 3, -100, -100, 0, 0, 0, 0, 0, -100, -100, 0, -100, -100]\n",
      "['<s>', '▁Ein', '▁64', '-', 'jährige', 'r', '▁Amerikan', 'er', '▁hatte', '▁dort', '▁am', '▁1.', '▁Oktober', '▁2017', '▁auf', '▁die', '▁Besucher', '▁eines', '▁Country', 'kon', 'zer', 'ts', '▁', 'geschoss', 'en', '▁und', '▁58', '▁von', '▁ihnen', '▁get', 'öt', 'et', '▁', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# check that the conversion worked.\n",
    "print(tokenized_dataset[\"train\"][0][\"labels\"])\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_dataset[\"train\"][0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79b234",
   "metadata": {},
   "source": [
    "**Define a DataCollator (for efficient padding of the tokens)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab6470d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f37d16",
   "metadata": {},
   "source": [
    "**Download the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "073f1432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# num labels is 4 bcz of the -100 label\n",
    "model = AutoModelForTokenClassification.from_pretrained(BASE_MODEL_NAME, num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc38168",
   "metadata": {},
   "source": [
    "***Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da3cbd7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/massey/envs/nbdev/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmystreamer\u001b[0m (\u001b[33mgoldfingerli\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/massey/ba_thesis/nb_ba/ORL/wandb/run-20230404_193807-iedvqf65</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/goldfingerli/erre_er_component/runs/iedvqf65' target=\"_blank\">peachy-water-4</a></strong> to <a href='https://wandb.ai/goldfingerli/erre_er_component' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/goldfingerli/erre_er_component' target=\"_blank\">https://wandb.ai/goldfingerli/erre_er_component</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/goldfingerli/erre_er_component/runs/iedvqf65' target=\"_blank\">https://wandb.ai/goldfingerli/erre_er_component/runs/iedvqf65</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [720/720 03:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.177652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>0.201397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=(\"wandb\" if  USE_WANDB else None),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52afef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: wandb in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (0.14.0)\n",
      "Requirement already satisfied: PyYAML in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: pathtools in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: typing-extensions in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: setproctitle in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from wandb) (40.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from wandb) (4.22.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from wandb) (1.19.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (6.1.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/massey/envs/nbdev/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "091dd7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case no model was loaded up until now.\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_path = TRAINED_MODEL_NAME\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5a4f64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_0',\n",
       "  'score': 0.9985538,\n",
       "  'index': 1,\n",
       "  'word': '▁Er',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9995691,\n",
       "  'index': 2,\n",
       "  'word': '▁sagt',\n",
       "  'start': 3,\n",
       "  'end': 7},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99906653,\n",
       "  'index': 3,\n",
       "  'word': ',',\n",
       "  'start': 7,\n",
       "  'end': 8},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9995834,\n",
       "  'index': 4,\n",
       "  'word': '▁dass',\n",
       "  'start': 9,\n",
       "  'end': 13},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9996511,\n",
       "  'index': 5,\n",
       "  'word': '▁der',\n",
       "  'start': 14,\n",
       "  'end': 17},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.9883938,\n",
       "  'index': 6,\n",
       "  'word': '▁Präsident',\n",
       "  'start': 18,\n",
       "  'end': 27},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99930274,\n",
       "  'index': 7,\n",
       "  'word': '▁dem',\n",
       "  'start': 28,\n",
       "  'end': 31},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.9624842,\n",
       "  'index': 8,\n",
       "  'word': '▁Volk',\n",
       "  'start': 32,\n",
       "  'end': 36},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9955988,\n",
       "  'index': 9,\n",
       "  'word': '▁etwas',\n",
       "  'start': 37,\n",
       "  'end': 42},\n",
       " {'entity': 'LABEL_3',\n",
       "  'score': 0.9922724,\n",
       "  'index': 10,\n",
       "  'word': '▁vor',\n",
       "  'start': 43,\n",
       "  'end': 46},\n",
       " {'entity': 'LABEL_3',\n",
       "  'score': 0.9150787,\n",
       "  'index': 11,\n",
       "  'word': 'ge',\n",
       "  'start': 46,\n",
       "  'end': 48},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.64520824,\n",
       "  'index': 12,\n",
       "  'word': 'macht',\n",
       "  'start': 48,\n",
       "  'end': 53},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99943167,\n",
       "  'index': 13,\n",
       "  'word': '▁hat',\n",
       "  'start': 54,\n",
       "  'end': 57},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9930059,\n",
       "  'index': 14,\n",
       "  'word': '.',\n",
       "  'start': 57,\n",
       "  'end': 58}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"token-classification\",\n",
    "                # model=trainer.model, -- in case freshly trained\n",
    "                model=model,\n",
    "                tokenizer=tokenizer)\n",
    "\n",
    "pipe(\"Er sagt, dass der Präsident dem Volk etwas vorgemacht hat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ef91e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_1',\n",
       "  'score': 0.98411083,\n",
       "  'index': 1,\n",
       "  'word': '▁Peter',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99857235,\n",
       "  'index': 2,\n",
       "  'word': '▁hat',\n",
       "  'start': 6,\n",
       "  'end': 9},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9982331,\n",
       "  'index': 3,\n",
       "  'word': '▁etwas',\n",
       "  'start': 10,\n",
       "  'end': 15},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99617213,\n",
       "  'index': 4,\n",
       "  'word': '▁gegen',\n",
       "  'start': 16,\n",
       "  'end': 21},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.79639655,\n",
       "  'index': 5,\n",
       "  'word': '▁Fritz',\n",
       "  'start': 22,\n",
       "  'end': 27},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99768126,\n",
       "  'index': 6,\n",
       "  'word': '!',\n",
       "  'start': 27,\n",
       "  'end': 28}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Peter hat etwas gegen Fritz!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cf615",
   "metadata": {},
   "source": [
    "**Evaluation on the test / val set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "113c1218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1383/1383 [00:00<00:00, 3739.28ex/s]\n",
      "100%|██████████| 1383/1383 [00:00<00:00, 3968.17ex/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenized dataset, convert numerical labels to labels ready for evaluation.\n",
    "\n",
    "def labelize(example):\n",
    "    labels1 = []\n",
    "    for label in example[\"labels\"]:\n",
    "        if label in [0, -100]:\n",
    "            labels1.append(\"LABEL_0\")\n",
    "        if label in [1]:\n",
    "            labels1.append(\"LABEL_1\")\n",
    "        if label in [2]:\n",
    "            labels1.append(\"LABEL_2\")\n",
    "        if label in [3]:\n",
    "            labels1.append(\"LABEL_3\")\n",
    "    example[\"labels\"] = labels1\n",
    "    return example\n",
    "\n",
    "def subword_ids_to_strings(example):\n",
    "    example[\"subword_tokens\"] = tokenizer.convert_ids_to_tokens(example[\"input_ids\"])\n",
    "    return example\n",
    "\n",
    "tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].map(labelize)\n",
    "tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].map(subword_ids_to_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c986161f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seine Besorgnis gelte vorerst vor allem den Austern- und Muschelkulturen .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# verify\n",
    "tokenizer.decode(tokenized_dataset[\"test\"][0][\"input_ids\"])\n",
    "len(tokenized_dataset[\"test\"])\n",
    "\n",
    "small_ds = tokenized_dataset[\"test\"].select(range(8))\n",
    "\n",
    "len(small_ds)\n",
    "\n",
    "\" \".join(small_ds[0][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc46aa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1383/1383 [17:46<00:00,  1.30it/s]\n",
      "/home/user/massey/envs/nbdev/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/user/massey/envs/nbdev/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/user/massey/envs/nbdev/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_3 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/user/massey/envs/nbdev/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_2 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ABEL_0       0.59      0.54      0.57      5086\n",
      "      ABEL_1       0.67      0.68      0.68      1383\n",
      "      ABEL_2       0.67      0.50      0.57      1383\n",
      "      ABEL_3       0.72      0.64      0.68      1383\n",
      "\n",
      "   micro avg       0.63      0.57      0.60      9235\n",
      "   macro avg       0.66      0.59      0.62      9235\n",
      "weighted avg       0.64      0.57      0.60      9235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "y_total_pred = []\n",
    "y_total_true = []\n",
    "\n",
    "for e in tqdm(tokenized_dataset[\"test\"]):\n",
    "    pred = [x[\"entity\"] for x in pipe(\" \".join(e[\"tokens\"]))]\n",
    "    # print(\"Prediction length:\", len(pred))\n",
    "    # print(\"Subword length:\", len(e[\"subword_tokens\"][1:-1]))\n",
    "    # print(\"Labels length:\", len(e[\"labels\"][1:-1]))\n",
    "    # print(\"----\")\n",
    "    y_total_pred.append(pred)\n",
    "    true = e[\"labels\"]\n",
    "    y_total_true.append(true[1:-1])\n",
    "\n",
    "print(classification_report(y_total_true, y_total_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19b514d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_1',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_3',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Waiting for W&B process to finish... (success).\n"
     ]
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe669a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
