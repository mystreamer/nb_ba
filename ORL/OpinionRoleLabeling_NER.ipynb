{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ecbf02",
   "metadata": {},
   "source": [
    "## Opinion Role Labeling\n",
    "\n",
    "### Using the NER approach\n",
    "-------\n",
    "\n",
    "**Requirements**\n",
    "```\n",
    "!pip install transformers datasets evaluate seqeval\n",
    "```\n",
    "\n",
    "alternatively use the following command (on *anaconda*):\n",
    "```\n",
    "!conda install -y -c conda-forge transformers datasets evaluate seqeval\n",
    "```\n",
    "\n",
    "**Load a dataset in the following format**\n",
    "```\n",
    "[CLS] Das immer wieder mit dem Separatismus liebäugelnde französischsprachige Quebec wollte sich mit dem Verfassungskompromiss , TARGET die neun vorwiegend englischsprachigen HOLDER schliesslich zugestimmt hatten , nicht abfinden . [SEP] \n",
    "[CLS] Das immer wieder mit dem Separatismus liebäugelnde französischsprachige Quebec wollte sich mit dem Verfassungskompromiss , dem die neun vorwiegend englischsprachigen Provinzen schliesslich zugestimmt hatten , nicht abfinden . [SEP] \n",
    "[CLS] Von den beiden grossen Gewerkschaften hatte die HOLDER ( Unión General del Trabajo ) dieses TARGET unterstützt , während die Comisiones Obreras ( CCOO ) es im Vorfeld abgelehnt hatten , sich jetzt aber mit dem Resultat abfinden . [SEP] \n",
    "[CLS] Von den beiden grossen Gewerkschaften hatte die UGT ( Unión General del Trabajo ) dieses Vorhaben unterstützt , während die Comisiones Obreras ( CCOO ) es im Vorfeld abgelehnt hatten , sich jetzt aber mit dem Resultat abfinden . [SEP] \n",
    "[CLS] Der HOLDER hat keine TARGET verletzt . » Petkovic lehnt laut italienischen Medien auch ein Angebot ab , mit 30 Prozent der Summe abgefunden zu werden . [SEP] \n",
    "[CLS] Der Trainer hat keine Regeln verletzt . » Petkovic lehnt laut italienischen Medien auch ein Angebot ab , mit 30 Prozent der Summe abgefunden zu werden . [SEP] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fe111bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG VARIABLES\n",
    "\n",
    "# DATASET_PATH=\"../../etl/data/raw/03_holder_target.txt\"\n",
    "\n",
    "TRAIN_DATASET_PATH=\"../../etl/data/processed/ORLConverter/01_train_orl.txt\"\n",
    "VAL_DATASET_PATH=\"../../etl/data/processed/ORLConverter/01_val_orl.txt\"\n",
    "TEST_DATASET_PATH=\"../../etl/data/processed/ORLConverter/01_test_orl.txt\"\n",
    "\n",
    "GENERATE_DATASET=True\n",
    "\n",
    "BASE_MODEL_NAME=\"xlm-roberta-base\"\n",
    "\n",
    "TRAINED_MODEL_NAME=\"./data/trained_model_xlm_roberta_base\"\n",
    "\n",
    "USE_WANDB=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a758b8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=erre_er_component\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=erre_er_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dbe029c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in the data: 7666\n",
      "Total rows in the data: 2744\n",
      "Total rows in the data: 2766\n"
     ]
    }
   ],
   "source": [
    "def read_sents(dataset_path):\n",
    "    sents = []\n",
    "\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for c, l in enumerate(f.readlines()):\n",
    "            if c < 100:\n",
    "                pass\n",
    "            sents.append(l.strip())\n",
    "\n",
    "    # Der Minister findet die Debatte langweilig.\n",
    "    real_sents = sents[1::2]\n",
    "\n",
    "    # Der HOLDER findet die TARGET langweilig.\n",
    "    masked_sents = sents[0::2]\n",
    "\n",
    "    assert len(real_sents) == len(masked_sents), \"Nr of masked and real sentences are not equal!\"\n",
    "\n",
    "    print(f\"Total rows in the data: {len(real_sents) + len(masked_sents)}\")\n",
    "    \n",
    "    return real_sents, masked_sents\n",
    "\n",
    "train_sents_real, train_sents_masked = read_sents(TRAIN_DATASET_PATH)\n",
    "val_sents_real, val_sents_masked = read_sents(VAL_DATASET_PATH)\n",
    "test_sents_real, test_sents_masked = read_sents(TEST_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dd38cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max holders:  1 Max targets:  1 Max polar expressions:  1\n",
      "Max holders:  1 Max targets:  1 Max polar expressions:  1\n",
      "Max holders:  1 Max targets:  1 Max polar expressions:  1\n",
      "[{'id': 0, 'ner_tags': [0, 1, 3, 0, 0, 2, 0, 0], 'tokens': ['Die', 'Grasshoppers', 'nutzten', 'den', 'personellen', 'Vorteil', 'aus', '.']}, {'id': 1, 'ner_tags': [0, 0, 1, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['Als', 'sich', 'Prinzessin', 'Haya', 'für', 'die', 'jungen', 'Frauen', 'eingesetzt', 'habe', ',', 'sei', 'auch', 'sie', 'ins', 'Visier', 'des', 'Emirs', 'geraten', 'und', 'mit', 'dem', 'Tod', 'bedroht', 'worden', '.']}, {'id': 2, 'ner_tags': [1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0], 'tokens': ['Maurer', 'erreichte', 'in', 'dem', 'Departement', ',', 'das', 'er', 'bis', '2015', 'führte', ',', 'eine', 'gewisse', 'Beruhigung', 'der', 'Verhältnisse', '.']}, {'id': 3, 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 2, 0], 'tokens': ['Allerdings', 'reagiert', 'er', 'trotzig', '–', 'und', 'das', 'macht', 'ihn', 'oft', 'nur', 'noch', 'stärker.Cécile', 'Klotzbach', 'Nach', 'jedem', 'Sieg', 'bedankt', 'sich', 'Djokovic', 'mit', 'dieser', 'Geste', 'bei', 'den', 'Fans', '.']}, {'id': 4, 'ner_tags': [0, 2, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['Den', 'Kraftwerksbetreibern', 'fehlen', 'derzeit', 'schlicht', 'die', 'notwendigen', 'Sicherheiten', ',', 'um', 'Investitionen', 'zu', 'tätigen', 'und', 'das', 'Projekt', 'voranzutreiben', '.']}, {'id': 5, 'ner_tags': [0, 3, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['Noch', 'schiesst', 'Jérémie', 'Heitz', 'auf', 'Ski', 'Gipfel', 'hinab', ',', 'die', 'andere', 'mit', 'Eispickeln', 'hochklettern', '.']}, {'id': 6, 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 3, 0], 'tokens': ['Diese', 'Menge', 'dürfte', 'sicherstellen', ',', 'dass', 'auch', 'bei', 'hohem', 'Impftempo', 'bis', 'Mitte', 'März', 'keine', 'Engpässe', 'mehr', 'entstehen', '.']}, {'id': 7, 'ner_tags': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0], 'tokens': ['Die', 'Schweizer', 'Politik', 'und', 'Diplomatie', 'sind', 'weiterhin', 'dringend', 'aufgerufen', ',', 'sich', 'mit', 'Geschick', 'und', 'Überzeugungskraft', 'für', 'einen', 'verdienten', 'Mitbürger', 'einzusetzen', '.']}, {'id': 8, 'ner_tags': [0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0], 'tokens': ['Die', 'Kreation', 'wirkte', 'wie', 'eine', 'mittelmässige', 'Abschlussarbeit', 'eines', 'traditionsbewussten', 'Grafik-Studenten', '–', 'aber', 'nur', 'auf', 'den', 'ersten', 'Blick', '.']}, {'id': 9, 'ner_tags': [0, 0, 3, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['Im', 'Februar', 'klagte', 'das', 'US-Justizdepartement', 'fünf', 'ehemalige', 'Kader', 'des', 'Erdölkonzerns', 'und', 'weitere', 'Personen', 'wegen', 'Bestechung', 'und', 'Geldwäscherei', 'an', '.']}]\n"
     ]
    }
   ],
   "source": [
    "NER_labels = [\"O\", \"HOLDER\", \"TARGET\", \"PEXP\"]\n",
    "\n",
    "test_sent = [\"Peter sowie Lucy mag Katzen sowie auch Hunde .\"]\n",
    "test_sent_masked = [\"HOLDER sowie HOLDER mag TARGET sowie auch TARGET .\"]\n",
    "\n",
    "def align_sentences(real_sentences, masked_sentences, NER_labels):\n",
    "    \"\"\"Given two sentences that have their word-level tokens delimited by a white-space, \n",
    "    create NER-tags for each sentence token.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    max_multiple_holders = 0\n",
    "    max_multiple_targets = 0\n",
    "    max_multiple_pexps = 0\n",
    "\n",
    "    for i, real_sentence, masked_sentence in zip(range(0,len(real_sentences)), real_sentences, masked_sentences):\n",
    "        ner_tags = []\n",
    "        real_sentence_split = real_sentence.split(\" \")\n",
    "        masked_sentence_split = masked_sentence.split(\" \")\n",
    "        \n",
    "        # target / holder counting\n",
    "        target_cnt = 0\n",
    "        holder_cnt = 0\n",
    "        pexp_cnt = 0\n",
    "\n",
    "        assert len(real_sentence_split) == len(masked_sentence_split), \"Misalignment of length of tokens in sentence.\" \\\n",
    "        + str(real_sentence_split) + str(masked_sentence_split) + str(i)\n",
    "\n",
    "        for real_token, masked_token in zip(real_sentence_split, masked_sentence_split):\n",
    "            if real_token == masked_token:\n",
    "                ner_tags.append(0)\n",
    "            elif masked_token == \"HOLDER\":\n",
    "                holder_cnt += 1\n",
    "                tag_index = NER_labels.index(masked_token)\n",
    "                ner_tags.append(tag_index)\n",
    "            elif masked_token == \"TARGET\":\n",
    "                target_cnt += 1\n",
    "                tag_index = NER_labels.index(masked_token)\n",
    "                ner_tags.append(tag_index)\n",
    "            elif masked_token == \"PEXP\":\n",
    "                pexp_cnt += 1\n",
    "                tag_index = NER_labels.index(masked_token)\n",
    "                ner_tags.append(tag_index)\n",
    "\n",
    "        dataset.append({\n",
    "            \"id\": i,\n",
    "            # remove the magic tokens from the sentences, \n",
    "            # since we will pass the sentences through the tokenizer again.\n",
    "            \"ner_tags\": ner_tags[1:-1],\n",
    "            \"tokens\": real_sentence_split[1:-1],\n",
    "        })\n",
    "    max_multiple_holders = max(max_multiple_holders, holder_cnt)\n",
    "    max_multiple_targets = max(max_multiple_targets, target_cnt)\n",
    "    max_multiple_pexps = max(max_multiple_pexps, pexp_cnt)\n",
    "\n",
    "    return dataset, max_multiple_holders, max_multiple_targets, max_multiple_pexps\n",
    "\n",
    "# align for training\n",
    "train_aligned_sents, mmh, mmt, mmp = align_sentences(train_sents_real, train_sents_masked, NER_labels)\n",
    "print(\"Max holders: \", mmh, \"Max targets: \", mmt, \"Max polar expressions: \", mmp)\n",
    "val_aligned_sents, mmh, mmt, mmp = align_sentences(val_sents_real, val_sents_masked, NER_labels)\n",
    "print(\"Max holders: \", mmh, \"Max targets: \", mmt, \"Max polar expressions: \", mmp)\n",
    "test_aligned_sents, mmh, mmt, mmp = align_sentences(test_sents_real, test_sents_masked, NER_labels)\n",
    "print(\"Max holders: \", mmh, \"Max targets: \", mmt, \"Max polar expressions: \", mmp)\n",
    "\n",
    "# UNCOMMENT FOR TEST\n",
    "# aligned_sents, mmh, mmt = align_sentences(test_sent, test_sent_masked, NER_labels)\n",
    "\n",
    "print(train_aligned_sents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24033875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf02381583ed4454b6ea70950e464f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431848c41d3a4855aa1bfa244b99ba85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23032be2480840749a04b27ef9f1830b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'ner_tags', 'tokens'],\n",
      "        num_rows: 3833\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['id', 'ner_tags', 'tokens'],\n",
      "        num_rows: 1372\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'ner_tags', 'tokens'],\n",
      "        num_rows: 1383\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Make a huggingface dataset out of the record-based dataset\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "\n",
    "if GENERATE_DATASET:\n",
    "    train_dataset = Dataset.from_list(train_aligned_sents)\n",
    "    val_dataset = Dataset.from_list(val_aligned_sents)\n",
    "    test_dataset = Dataset.from_list(test_aligned_sents)\n",
    "\n",
    "    # randomly shuffle\n",
    "    train_dataset = train_dataset.shuffle(seed=42)\n",
    "    val_dataset = val_dataset.shuffle(seed=42)\n",
    "    test_dataset = test_dataset.shuffle(seed=42)\n",
    "\n",
    "    # dataset = dataset.train_test_split(test_size=0.2, shuffle=False)\n",
    "    # train_test_dataset = dataset.train_test_split(test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Split the 10% test + valid in half test, half valid\n",
    "    # test_valid = train_test_dataset['test'].train_test_split(test_size=0.4, shuffle=False)\n",
    "\n",
    "    # gather everyone if you want to have a single DatasetDict\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'valid': val_dataset,\n",
    "        'test': test_dataset})\n",
    "\n",
    "    dataset.save_to_disk(\"./data/split_dataset.hf\")\n",
    "else:\n",
    "    dataset = load_from_disk(\"./data/split_dataset.hf\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe90f4b",
   "metadata": {},
   "source": [
    "**Use DistilBERT tokenizer and embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bac02eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ce9726e76040388c0ab4b8a28f1421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1d8818300444759aa5c722a845e8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a514f6a3780448dda7d0050585724e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f53981c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 2991, 7911, 9, 81405, 42, 45331, 56, 10264, 18370, 444, 615, 18146, 505, 644, 68, 78481, 9026, 74831, 1716, 4383, 933, 6, 190848, 33, 165, 9318, 542, 37616, 2046, 16463, 126, 6, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['<s>', '▁Ein', '▁64', '-', 'jährige', 'r', '▁Amerikan', 'er', '▁hatte', '▁dort', '▁am', '▁1.', '▁Oktober', '▁2017', '▁auf', '▁die', '▁Besucher', '▁eines', '▁Country', 'kon', 'zer', 'ts', '▁', 'geschoss', 'en', '▁und', '▁58', '▁von', '▁ihnen', '▁get', 'öt', 'et', '▁', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# tokenize input\n",
    "tokenized_input = tokenizer(dataset[\"train\"][0][\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "print(tokenized_input)\n",
    "\n",
    "# output subwords\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "005e2f43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c0408f211046938bc2fbf6b26c786f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed07b7f7df5406bb63a82c4ab4d0f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ba253d565244c5a19ca7c6aa67708b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'ner_tags', 'tokens', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3833\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'ner_tags', 'tokens', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1372\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'ner_tags', 'tokens', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1383\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        try:\n",
    "            for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                    label_ids.append(label[word_idx])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "            labels.append(label_ids)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping example due to the following: { str(e) }\")\n",
    "            print(\" \".join(examples[f\"tokens\"][i]))\n",
    "            print(label)\n",
    "            continue\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57b33056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 0, -100, -100, -100, 1, -100, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, -100, -100, -100, 3, -100, -100, 0, 0, 0, 0, 0, -100, -100, 0, -100, -100]\n",
      "['<s>', '▁Ein', '▁64', '-', 'jährige', 'r', '▁Amerikan', 'er', '▁hatte', '▁dort', '▁am', '▁1.', '▁Oktober', '▁2017', '▁auf', '▁die', '▁Besucher', '▁eines', '▁Country', 'kon', 'zer', 'ts', '▁', 'geschoss', 'en', '▁und', '▁58', '▁von', '▁ihnen', '▁get', 'öt', 'et', '▁', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# check that the conversion worked.\n",
    "print(tokenized_dataset[\"train\"][0][\"labels\"])\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_dataset[\"train\"][0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca79b234",
   "metadata": {},
   "source": [
    "**Define a DataCollator (for efficient padding of the tokens)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab6470d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f37d16",
   "metadata": {},
   "source": [
    "**Download the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "073f1432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b4c82795ab4edb9f8a6f6e9cc7924a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# num labels is 4 bcz of the -100 label\n",
    "model = AutoModelForTokenClassification.from_pretrained(BASE_MODEL_NAME, num_labels=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc38168",
   "metadata": {},
   "source": [
    "***Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da3cbd7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "WandbCallback requires wandb to be installed. Run `pip install wandb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/parallels/Desktop/Parallels Shared Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb Cell 17\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./data/results\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     report_to\u001b[39m=\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mwandb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m  USE_WANDB \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_args,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtokenized_dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mtokenized_dataset[\u001b[39m\"\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model(TRAINED_MODEL_NAME)\n",
      "File \u001b[0;32m~/envs/nbdev/lib64/python3.10/site-packages/transformers/trainer.py:479\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    477\u001b[0m default_callbacks \u001b[39m=\u001b[39m DEFAULT_CALLBACKS \u001b[39m+\u001b[39m get_reporting_integration_callbacks(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mreport_to)\n\u001b[1;32m    478\u001b[0m callbacks \u001b[39m=\u001b[39m default_callbacks \u001b[39mif\u001b[39;00m callbacks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m default_callbacks \u001b[39m+\u001b[39m callbacks\n\u001b[0;32m--> 479\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler \u001b[39m=\u001b[39m CallbackHandler(\n\u001b[1;32m    480\u001b[0m     callbacks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_scheduler\n\u001b[1;32m    481\u001b[0m )\n\u001b[1;32m    482\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_callback(PrinterCallback \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdisable_tqdm \u001b[39melse\u001b[39;00m DEFAULT_PROGRESS_CALLBACK)\n\u001b[1;32m    484\u001b[0m \u001b[39m# Will be set to True by `self._setup_loggers()` on first call to `self.log()`.\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/nbdev/lib64/python3.10/site-packages/transformers/trainer_callback.py:296\u001b[0m, in \u001b[0;36mCallbackHandler.__init__\u001b[0;34m(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks:\n\u001b[0;32m--> 296\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_callback(cb)\n\u001b[1;32m    297\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[1;32m    298\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tokenizer\n",
      "File \u001b[0;32m~/envs/nbdev/lib64/python3.10/site-packages/transformers/trainer_callback.py:313\u001b[0m, in \u001b[0;36mCallbackHandler.add_callback\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_callback\u001b[39m(\u001b[39mself\u001b[39m, callback):\n\u001b[0;32m--> 313\u001b[0m     cb \u001b[39m=\u001b[39m callback() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(callback, \u001b[39mtype\u001b[39m) \u001b[39melse\u001b[39;00m callback\n\u001b[1;32m    314\u001b[0m     cb_class \u001b[39m=\u001b[39m callback \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(callback, \u001b[39mtype\u001b[39m) \u001b[39melse\u001b[39;00m callback\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m cb_class \u001b[39min\u001b[39;00m [c\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks]:\n",
      "File \u001b[0;32m~/envs/nbdev/lib64/python3.10/site-packages/transformers/integrations.py:647\u001b[0m, in \u001b[0;36mWandbCallback.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m has_wandb \u001b[39m=\u001b[39m is_wandb_available()\n\u001b[1;32m    646\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m has_wandb:\n\u001b[0;32m--> 647\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWandbCallback requires wandb to be installed. Run `pip install wandb`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    648\u001b[0m \u001b[39mif\u001b[39;00m has_wandb:\n\u001b[1;32m    649\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: WandbCallback requires wandb to be installed. Run `pip install wandb`."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./data/results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=(\"wandb\" if  USE_WANDB else None),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52afef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: line 1: /home/parallels/envs/nbdev/bin/pip: cannot execute: required file not found\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091dd7be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAINED_MODEL_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/parallels/Desktop/Parallels Shared Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb Cell 17\u001b[0m in \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# In case no model was loaded up until now.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model_path \u001b[39m=\u001b[39m TRAINED_MODEL_NAME\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.211.55.5/home/parallels/Desktop/Parallels%20Shared%20Folders/Developer/ba/ba_thesis/nb_ba/ORL/OpinionRoleLabeling_NER.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForTokenClassification\u001b[39m.\u001b[39mfrom_pretrained(model_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TRAINED_MODEL_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "# In case no model was loaded up until now.\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_path = TRAINED_MODEL_NAME\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4f64c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_0',\n",
       "  'score': 0.99943393,\n",
       "  'index': 1,\n",
       "  'word': 'Er',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99932384,\n",
       "  'index': 2,\n",
       "  'word': 'sagt',\n",
       "  'start': 3,\n",
       "  'end': 7},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99940085,\n",
       "  'index': 3,\n",
       "  'word': ',',\n",
       "  'start': 7,\n",
       "  'end': 8},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99937755,\n",
       "  'index': 4,\n",
       "  'word': 'dass',\n",
       "  'start': 9,\n",
       "  'end': 13},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9993318,\n",
       "  'index': 5,\n",
       "  'word': 'der',\n",
       "  'start': 14,\n",
       "  'end': 17},\n",
       " {'entity': 'LABEL_1',\n",
       "  'score': 0.98201424,\n",
       "  'index': 6,\n",
       "  'word': 'Präsident',\n",
       "  'start': 18,\n",
       "  'end': 27},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9995322,\n",
       "  'index': 7,\n",
       "  'word': 'dem',\n",
       "  'start': 28,\n",
       "  'end': 31},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.8740551,\n",
       "  'index': 8,\n",
       "  'word': 'Volk',\n",
       "  'start': 32,\n",
       "  'end': 36},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.98325515,\n",
       "  'index': 9,\n",
       "  'word': 'etwas',\n",
       "  'start': 37,\n",
       "  'end': 42},\n",
       " {'entity': 'LABEL_3',\n",
       "  'score': 0.9126522,\n",
       "  'index': 10,\n",
       "  'word': 'vorge',\n",
       "  'start': 43,\n",
       "  'end': 48},\n",
       " {'entity': 'LABEL_3',\n",
       "  'score': 0.76627177,\n",
       "  'index': 11,\n",
       "  'word': '##macht',\n",
       "  'start': 48,\n",
       "  'end': 53},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9993148,\n",
       "  'index': 12,\n",
       "  'word': 'hat',\n",
       "  'start': 54,\n",
       "  'end': 57},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.99935335,\n",
       "  'index': 13,\n",
       "  'word': '.',\n",
       "  'start': 57,\n",
       "  'end': 58}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"token-classification\",\n",
    "                # model=trainer.model, -- in case freshly trained\n",
    "                model=model,\n",
    "                tokenizer=tokenizer)\n",
    "\n",
    "pipe(\"Er sagt, dass der Präsident dem Volk etwas vorgemacht hat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef91e33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'LABEL_1',\n",
       "  'score': 0.9424176,\n",
       "  'index': 1,\n",
       "  'word': 'Peter',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9950648,\n",
       "  'index': 2,\n",
       "  'word': 'hat',\n",
       "  'start': 6,\n",
       "  'end': 9},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9327351,\n",
       "  'index': 3,\n",
       "  'word': 'etwas',\n",
       "  'start': 10,\n",
       "  'end': 15},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9905109,\n",
       "  'index': 4,\n",
       "  'word': 'gegen',\n",
       "  'start': 16,\n",
       "  'end': 21},\n",
       " {'entity': 'LABEL_2',\n",
       "  'score': 0.8834155,\n",
       "  'index': 5,\n",
       "  'word': 'Fritz',\n",
       "  'start': 22,\n",
       "  'end': 27},\n",
       " {'entity': 'LABEL_0',\n",
       "  'score': 0.9602219,\n",
       "  'index': 6,\n",
       "  'word': '!',\n",
       "  'start': 27,\n",
       "  'end': 28}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Peter hat etwas gegen Fritz!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2cf615",
   "metadata": {},
   "source": [
    "**Evaluation on the test / val set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c1218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d68f2c8385b41b48d4e2eb7d8d853c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/933 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c3a28dad33430d8a7a949eaaf96df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/933 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenized dataset, convert numerical labels to labels ready for evaluation.\n",
    "\n",
    "def labelize(example):\n",
    "    labels1 = []\n",
    "    for label in example[\"labels\"]:\n",
    "        if label in [0, -100]:\n",
    "            labels1.append(\"LABEL_0\")\n",
    "        if label in [1]:\n",
    "            labels1.append(\"LABEL_1\")\n",
    "        if label in [2]:\n",
    "            labels1.append(\"LABEL_2\")\n",
    "        if label in [3]:\n",
    "            labels1.append(\"LABEL_3\")\n",
    "    example[\"labels\"] = labels1\n",
    "    return example\n",
    "\n",
    "def subword_ids_to_strings(example):\n",
    "    example[\"subword_tokens\"] = tokenizer.convert_ids_to_tokens(example[\"input_ids\"])\n",
    "    return example\n",
    "\n",
    "tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].map(labelize)\n",
    "tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].map(subword_ids_to_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986161f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vor allem der berüchtigten « Hölle des Nordens » von Paris nach Roubaix ( 17. April ) gilt seine Liebe .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# verify\n",
    "tokenizer.decode(tokenized_dataset[\"test\"][0][\"input_ids\"])\n",
    "len(tokenized_dataset[\"test\"])\n",
    "\n",
    "small_ds = tokenized_dataset[\"test\"].select(range(8))\n",
    "\n",
    "len(small_ds)\n",
    "\n",
    "\" \".join(small_ds[0][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc46aa04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 933/933 [00:30<00:00, 30.88it/s]\n",
      "/home/parallels/nbdev/lib64/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/parallels/nbdev/lib64/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_2 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/parallels/nbdev/lib64/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_3 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/parallels/nbdev/lib64/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LABEL_1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ABEL_0       0.66      0.69      0.68      3310\n",
      "      ABEL_1       0.73      0.82      0.77       933\n",
      "      ABEL_2       0.67      0.66      0.66       932\n",
      "      ABEL_3       0.83      0.88      0.86       933\n",
      "\n",
      "   micro avg       0.70      0.73      0.72      6108\n",
      "   macro avg       0.72      0.76      0.74      6108\n",
      "weighted avg       0.70      0.73      0.72      6108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "y_total_pred = []\n",
    "y_total_true = []\n",
    "\n",
    "for e in tqdm(tokenized_dataset[\"test\"]):\n",
    "    pred = [x[\"entity\"] for x in pipe(\" \".join(e[\"tokens\"]))]\n",
    "    # print(\"Prediction length:\", len(pred))\n",
    "    # print(\"Subword length:\", len(e[\"subword_tokens\"][1:-1]))\n",
    "    # print(\"Labels length:\", len(e[\"labels\"][1:-1]))\n",
    "    # print(\"----\")\n",
    "    y_total_pred.append(pred)\n",
    "    true = e[\"labels\"]\n",
    "    y_total_true.append(true[1:-1])\n",
    "\n",
    "print(classification_report(y_total_true, y_total_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b514d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LABEL_0',\n",
       " 'LABEL_1',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_3',\n",
       " 'LABEL_0',\n",
       " 'LABEL_2',\n",
       " 'LABEL_2',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0',\n",
       " 'LABEL_0']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe669a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
