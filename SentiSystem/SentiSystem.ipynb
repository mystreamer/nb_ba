{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4e241a",
   "metadata": {},
   "source": [
    "## SentiSystem\n",
    "\n",
    "This notebook should represent the combined approach of ORL (heads) and predicate extraction (assuming they are the subjectivity expression). \n",
    "\n",
    "Tag the sentence *ORL-model*.\n",
    "\n",
    "Extract predicate *Spacy-model*.\n",
    "\n",
    "Feed into the *SVC-Classifier*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11075f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG-VARIABLES\n",
    "SKLEARN_MODEL_PATH=\"./data/svc_relation_prediction.sav\"\n",
    "TRAINING_DATA_PATH=\"./data/training_data.csv\"\n",
    "FASTTEXT_MODEL_BIN_PATH=\"../../stancer_setup/models/cc.de.300.bin\"\n",
    "ORL_MODEL_PATH=\"../ORL/data/trained_model_german_bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4017ee84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/parallels/nbdev/lib/python3.10/site-packages (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: setuptools in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (1.23.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/parallels/nbdev/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/parallels/nbdev/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/parallels/nbdev/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/parallels/nbdev/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/parallels/nbdev/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "607aeaba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting de-core-news-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.4.0/de_core_news_sm-3.4.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from de-core-news-sm==3.4.0) (3.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: jinja2 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.23.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.10.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (59.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/parallels/nbdev/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/parallels/nbdev/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/parallels/nbdev/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/parallels/nbdev/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/parallels/nbdev/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.1.1)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.4.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_sm en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5630fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'LABEL_0', 'score': 0.9997595, 'word': 'Er sagt, dass der', 'start': 0, 'end': 17}, {'entity_group': 'LABEL_1', 'score': 0.98157835, 'word': 'Präsident', 'start': 18, 'end': 27}, {'entity_group': 'LABEL_0', 'score': 0.99997735, 'word': 'dem', 'start': 28, 'end': 31}, {'entity_group': 'LABEL_2', 'score': 0.54729056, 'word': 'Volk', 'start': 32, 'end': 36}, {'entity_group': 'LABEL_0', 'score': 0.99583834, 'word': 'etwas vorgemacht hat.', 'start': 37, 'end': 59}]\n"
     ]
    }
   ],
   "source": [
    "# Load the ORL model.\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "model_path = ORL_MODEL_PATH\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "pipe = pipeline(task=\"token-classification\",\n",
    "                # model=trainer.model, -- in case freshly trained\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                aggregation_strategy='simple')\n",
    "\n",
    "classified = pipe(\"Er sagt, dass der Präsident dem Volk etwas vorgemacht hat .\")\n",
    "\n",
    "print(classified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdce0241",
   "metadata": {},
   "source": [
    "**Extract Target-Holder-Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c17b1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class PAS(NamedTuple):\n",
    "    arg1: str\n",
    "    arg2: str\n",
    "    vLemma: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3f2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapper = {\n",
    "    \"LABEL_0\": \"NEUTRAL\",\n",
    "    \"LABEL_1\": \"HOLDER\",\n",
    "    \"LABEL_2\": \"TARGET\"\n",
    "}\n",
    "\n",
    "def extract_args(bert_output):\n",
    "    \"\"\"Obtain arguments from a dict-list of a BERT model.\"\"\"\n",
    "    # the holder is the first argument, the target is the second argument\n",
    "    arg1=None\n",
    "    arg2=None\n",
    "    for c in classified:\n",
    "        if label_mapper[c[\"entity_group\"]] == \"HOLDER\":\n",
    "            arg1=c[\"word\"]\n",
    "        elif label_mapper[c[\"entity_group\"]] == \"TARGET\":\n",
    "            arg2=c[\"word\"]\n",
    "        else:\n",
    "            pass\n",
    "    return arg1, arg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6602e6d",
   "metadata": {},
   "source": [
    "**Initialize dependency parsing with spacy.**\n",
    "\n",
    "Labels are described [here](https://www.linguistik.hu-berlin.de/de/institut/professuren/korpuslinguistik/mitarbeiter-innen/hagen/STTS_Tagset_Tiger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ded5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "nlp.add_pipe(\"conll_formatter\", last=True)\n",
    "\n",
    "# text = ('Der Minister prangert das Urteil an.')\n",
    "text = ('Der Minister prangert die missliche Lage an!')\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeb1874",
   "metadata": {},
   "source": [
    "**Install SpaCy CoNLL / To output stanced text in CoNLL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d50a0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: spacy_conll in /home/parallels/nbdev/lib/python3.10/site-packages (3.2.0)\n",
      "Requirement already satisfied: textacy in /home/parallels/nbdev/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: spacy>=3.0.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy_conll) (3.4.2)\n",
      "Requirement already satisfied: jellyfish>=0.8.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from textacy) (0.9.0)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from textacy) (0.12.0)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from textacy) (1.2.0)\n",
      "Requirement already satisfied: requests>=2.10.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from textacy) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from textacy) (1.23.4)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from textacy) (1.9.2)\n",
      "Requirement already satisfied: pyphen>=0.10.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from textacy) (0.13.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from textacy) (1.1.2)\n",
      "Requirement already satisfied: cachetools>=4.0.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from textacy) (5.2.0)\n",
      "Requirement already satisfied: catalogue~=2.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from textacy) (2.0.8)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from textacy) (2.8.8)\n",
      "Requirement already satisfied: tqdm>=4.19.6 in /home/parallels/nbdev/lib/python3.10/site-packages (from textacy) (4.64.1)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from cytoolz>=0.10.1->textacy) (0.12.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/parallels/nbdev/lib/python3.10/site-packages (from requests>=2.10.0->textacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/parallels/nbdev/lib/python3.10/site-packages (from requests>=2.10.0->textacy) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from requests>=2.10.0->textacy) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/parallels/nbdev/lib/python3.10/site-packages (from requests>=2.10.0->textacy) (2.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from scikit-learn>=0.19.0->textacy) (3.1.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (8.1.5)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (0.4.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (1.10.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (3.0.8)\n",
      "Requirement already satisfied: setuptools in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (59.6.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (0.6.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (3.0.10)\n",
      "Requirement already satisfied: jinja2 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (1.0.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (2.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from spacy>=3.0.1->spacy_conll) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/parallels/nbdev/lib/python3.10/site-packages (from packaging>=20.0->spacy>=3.0.1->spacy_conll) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from pathy>=0.3.5->spacy>=3.0.1->spacy_conll) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=3.0.1->spacy_conll) (4.4.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/parallels/nbdev/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.1->spacy_conll) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.1->spacy_conll) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.1->spacy_conll) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from jinja2->spacy>=3.0.1->spacy_conll) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy_conll textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d688838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   form  lemma upostag xpostag  \\\n",
      "0   1    Sie    sie    PRON    PPER   \n",
      "1   2    mag  mögen     AUX   VMFIN   \n",
      "2   3    ihn    ihn    PRON    PPER   \n",
      "3   4  nicht  nicht    PART  PTKNEG   \n",
      "4   5      !     --   PUNCT      $.   \n",
      "\n",
      "                                               feats  head deprel deps  \\\n",
      "0  Case=Nom|Gender=Fem|Number=Sing|Person=3|PronT...     2     sb    _   \n",
      "1  Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbF...     0   root    _   \n",
      "2  Case=Acc|Gender=Masc|Number=Sing|Person=3|Pron...     2     oa    _   \n",
      "3                                                  _     2     ng    _   \n",
      "4                                                  _     2  punct    _   \n",
      "\n",
      "            misc  \n",
      "0              _  \n",
      "1              _  \n",
      "2              _  \n",
      "3  SpaceAfter=No  \n",
      "4  SpaceAfter=No  \n"
     ]
    }
   ],
   "source": [
    "# generate conll\n",
    "\n",
    "from spacy_conll import init_parser\n",
    "\n",
    "nlp = init_parser(\"de_core_news_sm\",\n",
    "                  \"spacy\",\n",
    "                  ext_names={\"conll_pd\": \"pandas\"},\n",
    "                  conversion_maps={\"deprel\": {\"ROOT\": \"root\"}})\n",
    "\n",
    "doc = nlp('Sie mag ihn nicht!')\n",
    "\n",
    "print(doc._.pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf989f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import textacy\n",
    "# extract svo-triples for classifier\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# doc = nlp('He does not like Peter.')\n",
    "\n",
    "text_ext = textacy.extract.subject_verb_object_triples(doc)\n",
    "\n",
    "print([t for t in text_ext])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c643145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sie', 'mag', 'ihn', 'nicht', '!']\n"
     ]
    }
   ],
   "source": [
    "print ([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d7505c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sie PPER mag sb sie\n",
      "mag VMFIN mag ROOT mögen\n",
      "ihn PPER mag oa ihn\n",
      "nicht PTKNEG mag ng nicht\n",
      "! $. mag punct --\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print (token.text, token.tag_, token.head.text, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8b3f7",
   "metadata": {},
   "source": [
    "**Approach 1: Search for separated verbs.**\n",
    "\n",
    "Pattern syntax is described [here]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b2a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "pattern_separated_verbs = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"main\",\n",
    "        \"RIGHT_ATTRS\": {\"TAG\" : {\"IN\": [\"VVFIN\", \"VVINF\", \"VVPP\"]}}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"main\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"zusatz\",\n",
    "        \"RIGHT_ATTRS\": {\"TAG\" : \"PTKVZ\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "simple_pattern = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"main\",\n",
    "        \"RIGHT_ATTRS\": {\"TAG\" : {\"IN\": [\"VVFIN\", \"VVINF\", \"VVPP\"]}}\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "separable_matcher = DependencyMatcher(nlp.vocab)\n",
    "separable_matcher.add(\"SEPARATED_VERBS\", [pattern_separated_verbs])\n",
    "\n",
    "simple_matcher = DependencyMatcher(nlp.vocab)\n",
    "simple_matcher.add(\"SIMPLE_VERB\", [simple_pattern])\n",
    "\n",
    "def obtain_predicate(sent_string):\n",
    "    \"\"\"Obtain the predicate from a sentence string.\"\"\"\n",
    "    pred = \"\"\n",
    "\n",
    "    text = (sent_string)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for token in doc:\n",
    "        print(token.text, token.tag_, token.head.text, token.dep_, token.lemma_)\n",
    "    \n",
    "    matches = separable_matcher(doc)\n",
    "    print(matches)\n",
    "\n",
    "    # only focus on non-nested statements\n",
    "    if len(matches) == 1:\n",
    "        # extract and put together predicate\n",
    "        for e in matches[0][1][::-1]:\n",
    "            pred += doc[e].lemma_\n",
    "\n",
    "    matches = simple_matcher(doc)\n",
    "    print(matches)\n",
    "\n",
    "    if pred == \"\" and len(matches) == 1:\n",
    "        # standard predicate\n",
    "        pred = f\"{doc[matches[0][1][0]].lemma_}\"\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a29bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function doing the heavy lifting, here we assume one relation per sentence (no compositionality).\n",
    "def extract_pas(sentence_string, verb):\n",
    "    # sentence with holder / target labels\n",
    "    labelled_sentence = pipe(sentence_string)\n",
    "    # obtain arguments from labels\n",
    "    arg1, arg2 = extract_args(labelled_sentence)\n",
    "    # obtain predicate\n",
    "    # pred = obtain_predicate(sentence_string)\n",
    "    pred = verb\n",
    "    return PAS(arg1, arg2, pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a1be3e",
   "metadata": {},
   "source": [
    "**Load the SVC classifier with the pro/con relations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a1e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "\n",
    "def load_fasttext_embeddings_from_pas(filepath, pas_list):\n",
    "    # verb embeddings\n",
    "    model = fasttext.load_model(filepath)\n",
    "    \n",
    "    vEmbs = [model[w] for w in list([pas.vLemma for pas in pas_list])]\n",
    "    # get the embeddings for the NP heads\n",
    "    args1_np_head = [model[w] for w in list([pas.arg1 for pas in pas_list])]\n",
    "    args2_np_head = [model[w] for w in list([pas.arg2 for pas in pas_list])]\n",
    "    # clear from memory\n",
    "    del model\n",
    "    return args1_np_head, args2_np_head, vEmbs\n",
    "\n",
    "def make_features(args1, args2, vEmbs):\n",
    "    # Horizontally concatenate the embeddings for each training instance.\n",
    "    for i in range(0, len(args1)):\n",
    "        if i == 0:\n",
    "            X = np.concatenate((args1[i], args2[i], vEmbs[i]))\n",
    "        else:\n",
    "            X = np.vstack((X, np.concatenate((args1[i], args2[i], vEmbs[i]))))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a1aa08d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral    21252\n",
      "pro         3320\n",
      "con         2427\n",
      "Name: rel_type, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_num</th>\n",
       "      <th>verb_form</th>\n",
       "      <th>verb_lemma</th>\n",
       "      <th>arg1</th>\n",
       "      <th>arg1_pos</th>\n",
       "      <th>arg1_head</th>\n",
       "      <th>arg2</th>\n",
       "      <th>arg2_pos</th>\n",
       "      <th>arg2_head</th>\n",
       "      <th>rel_type</th>\n",
       "      <th>pred_serial</th>\n",
       "      <th>full_sentence_text</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>verbessern</td>\n",
       "      <td>verbessern</td>\n",
       "      <td>Ein Inkrafttreten des Gegenentwurfs</td>\n",
       "      <td>N</td>\n",
       "      <td>Inkrafttreten</td>\n",
       "      <td>die Situation</td>\n",
       "      <td>N</td>\n",
       "      <td>Situation</td>\n",
       "      <td>pro</td>\n",
       "      <td>Predicate(type='pro', args=(Head(sentence=23, ...</td>\n",
       "      <td>Ein Inkrafttreten des Gegenentwurfs wird die S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>durchsetzen</td>\n",
       "      <td>durchsetzen</td>\n",
       "      <td>Die Initiative</td>\n",
       "      <td>N</td>\n",
       "      <td>Initiative</td>\n",
       "      <td>ein vollständiges Werbeverbot für Tabak</td>\n",
       "      <td>N</td>\n",
       "      <td>Werbeverbot</td>\n",
       "      <td>pro</td>\n",
       "      <td>Predicate(type='pro', args=(Head(sentence=9, t...</td>\n",
       "      <td>Die Initiative will jedoch auf versteckte Weis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>verbieten</td>\n",
       "      <td>verbieten</td>\n",
       "      <td>Der Bundesrat</td>\n",
       "      <td>N</td>\n",
       "      <td>Bundesrat</td>\n",
       "      <td>die Tabakwerbung</td>\n",
       "      <td>N</td>\n",
       "      <td>Tabakwerbung</td>\n",
       "      <td>con</td>\n",
       "      <td>Predicate(type='con', args=(Head(sentence=2, t...</td>\n",
       "      <td>Der Bundesrat will nun die Tabakwerbung in Kin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>verbieten</td>\n",
       "      <td>verbieten</td>\n",
       "      <td>der Bundesrat</td>\n",
       "      <td>N</td>\n",
       "      <td>Bundesrat</td>\n",
       "      <td>das Verteilen von Gratismustern</td>\n",
       "      <td>N</td>\n",
       "      <td>Verteilen</td>\n",
       "      <td>con</td>\n",
       "      <td>Predicate(type='con', args=(Head(sentence=13, ...</td>\n",
       "      <td>Allerdings will der Bundesrat das Verteilen vo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>unterstütze</td>\n",
       "      <td>unterstützen</td>\n",
       "      <td>die Wirtschaft</td>\n",
       "      <td>N</td>\n",
       "      <td>Wirtschaft</td>\n",
       "      <td>ein nationales Verkaufsverbot</td>\n",
       "      <td>N</td>\n",
       "      <td>Verkaufsverbot</td>\n",
       "      <td>pro</td>\n",
       "      <td>Predicate(type='pro', args=(Head(sentence=9, t...</td>\n",
       "      <td>Ebenso unterstütze die Wirtschaft zusätzlich e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20630</th>\n",
       "      <td>8536</td>\n",
       "      <td>sank</td>\n",
       "      <td>sinken</td>\n",
       "      <td>der Anteil der über 65-Jährigen</td>\n",
       "      <td>N</td>\n",
       "      <td>Anteil</td>\n",
       "      <td>22 Prozent</td>\n",
       "      <td>N</td>\n",
       "      <td>Prozent</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Predicate(type='neutral', args=(Head(sentence=...</td>\n",
       "      <td>Beispielsweise bei der Studie in England sank ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20631</th>\n",
       "      <td>8536</td>\n",
       "      <td>bleiben</td>\n",
       "      <td>bleiben</td>\n",
       "      <td>Demenz</td>\n",
       "      <td>N</td>\n",
       "      <td>Demenz</td>\n",
       "      <td>eine grosse Herausforderung für die alternden ...</td>\n",
       "      <td>N</td>\n",
       "      <td>Herausforderung</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Predicate(type='neutral', args=(Head(sentence=...</td>\n",
       "      <td>Das wäre ein grosser Fehler , findet ­ Monique...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20632</th>\n",
       "      <td>8536</td>\n",
       "      <td>ist</td>\n",
       "      <td>sein</td>\n",
       "      <td>Die Evidenz</td>\n",
       "      <td>N</td>\n",
       "      <td>Evidenz</td>\n",
       "      <td>ziemlich schwach</td>\n",
       "      <td>ADV</td>\n",
       "      <td>schwach</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Predicate(type='neutral', args=(Head(sentence=...</td>\n",
       "      <td>Die Evidenz ist immer noch ziemlich schwach » ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20633</th>\n",
       "      <td>8536</td>\n",
       "      <td>schützen</td>\n",
       "      <td>schützen</td>\n",
       "      <td>Bessere Lebensbedingungen und mehr Bildung</td>\n",
       "      <td>N</td>\n",
       "      <td>Lebensbedingungen</td>\n",
       "      <td>Bessere Lebensbedingungen und mehr Bildung</td>\n",
       "      <td>N</td>\n",
       "      <td>Lebensbedingungen</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Predicate(type='neutral', args=(Head(sentence=...</td>\n",
       "      <td>Bessere Lebensbedingungen und mehr Bildung sch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20634</th>\n",
       "      <td>8536</td>\n",
       "      <td>erwarten</td>\n",
       "      <td>erwarten</td>\n",
       "      <td>eine massive Zunahme</td>\n",
       "      <td>N</td>\n",
       "      <td>Zunahme</td>\n",
       "      <td>die Schweiz</td>\n",
       "      <td>N</td>\n",
       "      <td>Schweiz</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Predicate(type='neutral', args=(Head(sentence=...</td>\n",
       "      <td>In Ländern mit mittlerem oder tiefem Einkommen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13861 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_num    verb_form    verb_lemma  \\\n",
       "0            0   verbessern    verbessern   \n",
       "1            0  durchsetzen   durchsetzen   \n",
       "2            3    verbieten     verbieten   \n",
       "3            3    verbieten     verbieten   \n",
       "4            3  unterstütze  unterstützen   \n",
       "...        ...          ...           ...   \n",
       "20630     8536         sank        sinken   \n",
       "20631     8536      bleiben       bleiben   \n",
       "20632     8536          ist          sein   \n",
       "20633     8536     schützen      schützen   \n",
       "20634     8536     erwarten      erwarten   \n",
       "\n",
       "                                             arg1 arg1_pos          arg1_head  \\\n",
       "0             Ein Inkrafttreten des Gegenentwurfs        N      Inkrafttreten   \n",
       "1                                  Die Initiative        N         Initiative   \n",
       "2                                   Der Bundesrat        N          Bundesrat   \n",
       "3                                   der Bundesrat        N          Bundesrat   \n",
       "4                                  die Wirtschaft        N         Wirtschaft   \n",
       "...                                           ...      ...                ...   \n",
       "20630             der Anteil der über 65-Jährigen        N             Anteil   \n",
       "20631                                      Demenz        N             Demenz   \n",
       "20632                                 Die Evidenz        N            Evidenz   \n",
       "20633  Bessere Lebensbedingungen und mehr Bildung        N  Lebensbedingungen   \n",
       "20634                        eine massive Zunahme        N            Zunahme   \n",
       "\n",
       "                                                    arg2 arg2_pos  \\\n",
       "0                                          die Situation        N   \n",
       "1                ein vollständiges Werbeverbot für Tabak        N   \n",
       "2                                       die Tabakwerbung        N   \n",
       "3                        das Verteilen von Gratismustern        N   \n",
       "4                          ein nationales Verkaufsverbot        N   \n",
       "...                                                  ...      ...   \n",
       "20630                                         22 Prozent        N   \n",
       "20631  eine grosse Herausforderung für die alternden ...        N   \n",
       "20632                                   ziemlich schwach      ADV   \n",
       "20633         Bessere Lebensbedingungen und mehr Bildung        N   \n",
       "20634                                        die Schweiz        N   \n",
       "\n",
       "               arg2_head rel_type  \\\n",
       "0              Situation      pro   \n",
       "1            Werbeverbot      pro   \n",
       "2           Tabakwerbung      con   \n",
       "3              Verteilen      con   \n",
       "4         Verkaufsverbot      pro   \n",
       "...                  ...      ...   \n",
       "20630            Prozent  neutral   \n",
       "20631    Herausforderung  neutral   \n",
       "20632            schwach  neutral   \n",
       "20633  Lebensbedingungen  neutral   \n",
       "20634            Schweiz  neutral   \n",
       "\n",
       "                                             pred_serial  \\\n",
       "0      Predicate(type='pro', args=(Head(sentence=23, ...   \n",
       "1      Predicate(type='pro', args=(Head(sentence=9, t...   \n",
       "2      Predicate(type='con', args=(Head(sentence=2, t...   \n",
       "3      Predicate(type='con', args=(Head(sentence=13, ...   \n",
       "4      Predicate(type='pro', args=(Head(sentence=9, t...   \n",
       "...                                                  ...   \n",
       "20630  Predicate(type='neutral', args=(Head(sentence=...   \n",
       "20631  Predicate(type='neutral', args=(Head(sentence=...   \n",
       "20632  Predicate(type='neutral', args=(Head(sentence=...   \n",
       "20633  Predicate(type='neutral', args=(Head(sentence=...   \n",
       "20634  Predicate(type='neutral', args=(Head(sentence=...   \n",
       "\n",
       "                                      full_sentence_text  counts  \n",
       "0      Ein Inkrafttreten des Gegenentwurfs wird die S...       1  \n",
       "1      Die Initiative will jedoch auf versteckte Weis...       1  \n",
       "2      Der Bundesrat will nun die Tabakwerbung in Kin...       1  \n",
       "3      Allerdings will der Bundesrat das Verteilen vo...       1  \n",
       "4      Ebenso unterstütze die Wirtschaft zusätzlich e...       1  \n",
       "...                                                  ...     ...  \n",
       "20630  Beispielsweise bei der Studie in England sank ...       1  \n",
       "20631  Das wäre ein grosser Fehler , findet ­ Monique...       1  \n",
       "20632  Die Evidenz ist immer noch ziemlich schwach » ...       1  \n",
       "20633  Bessere Lebensbedingungen und mehr Bildung sch...       1  \n",
       "20634  In Ländern mit mittlerem oder tiefem Einkommen...       1  \n",
       "\n",
       "[13861 rows x 13 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- CLEAN ---\n",
    "\n",
    "training_data = pd.read_csv(TRAINING_DATA_PATH)\n",
    "\n",
    "print(training_data.rel_type.value_counts())\n",
    "\n",
    "# count duplicates (assumes multiple PAS in a given sentence)\n",
    "counts_per_sent = training_data.groupby(['full_sentence_text']).size().reset_index(name='counts').sort_values(by='counts', ascending=False)\n",
    "\n",
    "# print(counts_per_sent)\n",
    "\n",
    "# drop duplicates by sentence\n",
    "training_data_dedup = training_data.drop_duplicates(subset=['full_sentence_text'])\n",
    "\n",
    "# counts_per_sent.info()\n",
    "# training_data_dedup.info()\n",
    "\n",
    "# join new with counts\n",
    "joined_training_data = pd.merge(training_data_dedup, counts_per_sent, on='full_sentence_text')\n",
    "\n",
    "# print(joined_training_data)\n",
    "\n",
    "# select only sentences with a count of 1 PAS.\n",
    "simplified_training_data = joined_training_data[joined_training_data[\"counts\"] == 1]\n",
    "\n",
    "# simplified_training_data = simplified_training_data[simplified_training_data[\"rel_type\"] == \"pro\"]\n",
    "\n",
    "simplified_training_data = simplified_training_data[(simplified_training_data.arg1_head != \".\") & (simplified_training_data.arg2_head != \".\")]\n",
    "\n",
    "simplified_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332deea8",
   "metadata": {},
   "source": [
    "**Prepare data for experiment #1, details can be found here: [R-BERT](https://github.com/monologg/R-BERT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "adaa8df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.6.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_aarch64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/parallels/nbdev/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/parallels/nbdev/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: numpy>=1.19 in /home/parallels/nbdev/lib/python3.10/site-packages (from matplotlib) (1.23.4)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (278 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.9/278.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /home/parallels/nbdev/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/parallels/nbdev/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.0.6 cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.6.2 pillow-9.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e2454ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu(e1, e2)    10006\n",
       "pro(e1, e2)     2295\n",
       "con(e1, e2)     1560\n",
       "Name: rel_type, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simplified_training_data\n",
    "import re\n",
    "import copy\n",
    "\n",
    "def generate_head_based_entity_sentences(df):\n",
    "    entified_list = []\n",
    "    for t in df.itertuples():\n",
    "        sent = copy.deepcopy(t.full_sentence_text)\n",
    "        \n",
    "        try:\n",
    "            res = re.search(t.arg1_head, t.full_sentence_text)\n",
    "            e1_s, e1_e = res.span()\n",
    "            entified_1 = sent[0:max(0, e1_s - 1)] + \" <e1> \" + sent[e1_s:min(len(sent), e1_e + 1)] + \"</e1> \" + sent[min(len(sent), e1_e + 1):]\n",
    "\n",
    "            res = re.search(t.arg2_head, entified_1)\n",
    "            e2_s, e2_e = res.span()\n",
    "            entified_2 = entified_1[0:max(0, e2_s - 1)] + \" <e2> \" + entified_1[e2_s:min(len(entified_1), e2_e + 1)] + \"</e2> \" + entified_1[min(len(entified_1), e2_e + 1):]\n",
    "\n",
    "        except Exception as e:\n",
    "            if not res:\n",
    "                entified_list.append(None)\n",
    "                continue\n",
    "\n",
    "        entified_list.append(entified_2)\n",
    "    return entified_list\n",
    "\n",
    "entified_list = generate_head_based_entity_sentences(simplified_training_data)\n",
    "\n",
    "simplified_training_data[\"entified_sents\"] = entified_list\n",
    "\n",
    "# simplified_training_data[\"rel_type\"] = simplified_training_data[\"rel_type\"].map({\n",
    "#    \"pro\": \"pro(e1, e2)\",\n",
    "#    \"con\": \"con(e1, e2)\"\n",
    "# })\n",
    "\n",
    "reduced_training_data = simplified_training_data[[\"rel_type\", \"entified_sents\", \"arg1_head\", \"arg2_head\", \"full_sentence_text\"]].copy(deep=True)\n",
    "\n",
    "reduced_training_data.rel_type = simplified_training_data.rel_type.replace(to_replace=dict(pro=\"pro(e1, e2)\", con=\"con(e1, e2)\", neutral=\"neu(e1, e2)\"))\n",
    "\n",
    "reduced_training_data.rel_type.value_counts()\n",
    "\n",
    "# reduced_training_data.to_csv(\"./data/experiment_1_training_data.tsv\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f988e370",
   "metadata": {},
   "source": [
    "**Experiment 0: Run the basic classifier**\n",
    "\n",
    "TODO: Require some method to extract the verb (lemma). SVO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CLASSIFY ---\n",
    "\n",
    "pas_structures = [extract_pas(p, v) for p, v in zip(training_data.full_sentence_text.to_list(), training_data.verb_lemma.to_list())]\n",
    "\n",
    "args1_np_head, args2_np_head, vEmbs = load_fasttext_embeddings_from_pas(FASTTEXT_MODEL_BIN_PATH, pas_structures)\n",
    "\n",
    "X_np_args = make_features(args1_np_head, vEmbs, args2_np_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sklearn label encoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = numpy.load('classes.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7276865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 2 0 0 1 1 1 1]\n",
      "['pro', 'pro', 'con', 'pro', 'con', 'pro', 'pro', 'pro', 'pro', 'pro']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parallels/nbdev/lib64/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator SVC from version 1.0.2 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing, svm\n",
    "import pickle\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(SKLEARN_MODEL_PATH, 'rb'))\n",
    "\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "\n",
    "preds = loaded_model.predict(X_np_args)\n",
    "\n",
    "print(preds)\n",
    "\n",
    "print(training_data.rel_type.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88cd40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
